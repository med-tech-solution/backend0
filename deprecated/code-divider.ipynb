{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage,ToolMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language\n",
    "from langchain_together import ChatTogether,Together\n",
    "from together import Together as TogetherOG\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Literal, List, Union\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangchainJSONEngine:\n",
    "    def __init__(self, sampleBaseModel: BaseModel, systemPromptText: str=None, humanPromptText: str=None):\n",
    "        self.llm = llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "        self.structured_llm = llm.with_structured_output(sampleBaseModel)\n",
    "        \n",
    "        if systemPromptText is None:\n",
    "            self.systemPromptText = \"\"\"\n",
    "            You are an AI assistant. You are helping a user with a task. The user is asking you questions and you are answering them.\n",
    "            \"\"\"\n",
    "        else:\n",
    "            self.systemPromptText = systemPromptText\n",
    "\n",
    "        if humanPromptText is None:\n",
    "            self.HumanPromptText = \"\"\"\n",
    "            Human: {query}\n",
    "            \"\"\"\n",
    "        else:\n",
    "            self.humanPromptText = humanPromptText\n",
    "\n",
    "        self.prompt = ChatPromptTemplate.from_messages(\n",
    "            [(\"system\", self.systemPromptText), (\"human\", \"Query:\\n\\n {query}\")]\n",
    "            )\n",
    "        \n",
    "        self.micro_agent = self.prompt | self.structured_llm\n",
    "\n",
    "    def run(self, query: str):\n",
    "        result = self.micro_agent.invoke({\n",
    "            \"query\": query\n",
    "        }) \n",
    "        return result\n",
    "    \n",
    "\n",
    "class LangchainSimpleEngine:\n",
    "    def __init__(self, tools:List[tool]=[], systemPromptText: str=None, humanPromptText: str=None):\n",
    "        self.llm = llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "        self.tools = tools\n",
    "        \n",
    "        if len(tools) == 0:\n",
    "            self.llm_with_tools = llm\n",
    "        else:\n",
    "            self.llm_with_tools = llm.bind_tools(tools)\n",
    "            \n",
    "        if systemPromptText is None:\n",
    "            self.systemPromptText = \"\"\"\n",
    "            You are an AI assistant. You are helping a user with a task. The user is asking you questions and you are answering them.\n",
    "            \"\"\"\n",
    "        else:\n",
    "            self.systemPromptText = systemPromptText\n",
    "\n",
    "        if humanPromptText is not None: \n",
    "            print(\"Skipping human prompt text ...\")\n",
    "\n",
    "    def run(self, query: str):\n",
    "        messages = [\n",
    "            SystemMessage(self.systemPromptText),\n",
    "            HumanMessage(content=query)\n",
    "        ]\n",
    "        level1_result = self.llm_with_tools.invoke(messages)\n",
    "        if len(level1_result.tool_calls) == 0:\n",
    "            print(\"No tools to run ...\")\n",
    "            return level1_result\n",
    "        else:\n",
    "            print(\"Running tools ...\")\n",
    "            for tool_call in level1_result.tool_calls:\n",
    "                tool_output = tool_call.invoke()\n",
    "                messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "            level2_result = self.llm_with_tools.invoke(messages)\n",
    "            return level2_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_functions_and_classes(filepath):\n",
    "    \"\"\"\n",
    "    It takes a file path and returns the original source code of functions and code-snippets in the file\n",
    "    Code snippets are the source code of functions and classes in the file\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        file_content = file.read()\n",
    "    \n",
    "    # Parse the file content into an Abstract Syntax Tree (AST)\n",
    "    tree = ast.parse(file_content)\n",
    "    \n",
    "    # List to store the source code of functions and classes\n",
    "    code_snippets = {\n",
    "        \"classes\": [],\n",
    "        \"functions\": []\n",
    "    }\n",
    "\n",
    "    # Walk through the AST and find all functions and classes\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.ClassDef):\n",
    "            start_line = node.lineno - 1\n",
    "            end_line = node.end_lineno\n",
    "            code_snippets[\"classes\"].append(\"\".join(file_content.splitlines(keepends=True)[start_line:end_line]))\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            start_line = node.lineno - 1\n",
    "            end_line = node.end_lineno\n",
    "            code_snippets[\"functions\"].append(\"\".join(file_content.splitlines(keepends=True)[start_line:end_line]))\n",
    "    return file_content , code_snippets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code LLAMA Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_llama_prompt_formatter(query: str, system_prompt: str=None):\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "    if system_prompt is None:\n",
    "        SYSTEM_PROMPT = \"\"\"You are helpful coding assistant. User is asking you to write a function or class for a specific task. \n",
    "        Write the function or class in the programming language specified in the query.\"\"\"\n",
    "    else:\n",
    "        SYSTEM_PROMPT = system_prompt\n",
    "\n",
    "    USER_INSTRUCTION = f\"User: {query}\"\n",
    "    \n",
    "    SYSTEM_PROMPT = B_SYS + SYSTEM_PROMPT + E_SYS\n",
    "    PROMPT = B_INST + SYSTEM_PROMPT + USER_INSTRUCTION + E_INST\n",
    "    return PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_clean_code(s):\n",
    "    # Extract text between ``` and ```\n",
    "    pattern = r'```(.*?)```'\n",
    "    matches = re.findall(pattern, s, re.DOTALL)\n",
    "    \n",
    "    # Remove \"import logging\" from each extracted code block\n",
    "    cleaned_matches = [re.sub(r'\\bimport logging\\b', '', match).strip() for match in matches]\n",
    "    \n",
    "    return cleaned_matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Code LLama Instruct Engine\n",
    "class CodeLLamaInstructEngine:\n",
    "    def __init__(self, systemPromptText: str=None, paramcount: int=7):\n",
    "        self.AVL_PARAMS = [7,13,34]\n",
    "        if paramcount not in self.AVL_PARAMS:\n",
    "            raise ValueError(f\"Invalid paramcount. Choose from {self.AVL_PARAMS}\")\n",
    "        self.paramcount = paramcount\n",
    "        self.client = TogetherOG(api_key=os.environ.get('TOGETHER_API_KEY'))\n",
    "        self.model = f\"codellama/CodeLlama-{self.paramcount}b-Instruct-hf\"\n",
    "        if systemPromptText is None:\n",
    "            self.systemPromptText = \"\"\"\n",
    "            You are an AI assistant. You are helping a user with a task. The user is asking you to write a function or class for a specific task.\n",
    "            \"\"\"\n",
    "        else:\n",
    "            self.systemPromptText = systemPromptText\n",
    "        \n",
    "    def run(self, query: str, clean_code: bool=True):\n",
    "        PROMPT = code_llama_prompt_formatter(\n",
    "            system_prompt=self.systemPromptText,\n",
    "            query=query\n",
    "        )\n",
    "        response = self.client.completions.create(model=self.model, prompt=PROMPT)\n",
    "        response_text = response.choices[0].text\n",
    "        if clean_code:\n",
    "            return extract_and_clean_code(response_text)\n",
    "        else:\n",
    "            return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Embedding Workflow  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_code_in_file(file_content, old_code, new_code, handle_indentation=True):\n",
    "    if not handle_indentation:\n",
    "        return file_content.replace(old_code, new_code)\n",
    "    \n",
    "    # Count left side whitespaces in the old code\n",
    "    left_spaces = len(old_code) - len(old_code.lstrip())\n",
    "    # Add left spaces at the beginning of each line of the new code\n",
    "    new_code = \"\\n\".join([f\"{' '*left_spaces}{line}\" for line in new_code.splitlines()])\n",
    "    # Replace old code with new code\n",
    "    new_file_content = file_content.replace(old_code, new_code)\n",
    "    return new_file_content\n",
    "\n",
    "def read_code(filepath):\n",
    "    with open(filepath, \"r\") as file:\n",
    "        return file.read()\n",
    "    \n",
    "\n",
    "def write_code(filepath, content):\n",
    "    with open(filepath, \"w\") as file:\n",
    "        file.write(content)\n",
    "    \n",
    "def hash(s):\n",
    "    return sum([ord(c) for c in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogEmbedderWorkflow:\n",
    "    def __init__(self,paramcount:int=7):\n",
    "        self.embedder_code_llama_engine = CodeLLamaInstructEngine(\n",
    "                systemPromptText=\"\"\"\n",
    "                You are an AI assistant. You will be given python function code snippet and a log message.\n",
    "                Firstly you need to add logging at the beginning of the function body.\n",
    "                Then you need to add logging BEFORE all 'return statements' in the function.\n",
    "                If there is NO return statement, add logging at the end of the function.\n",
    "\n",
    "                Don't add any logging after the return statement, always add BEFORE the return statement.\n",
    "                Don't modify the function code snippet. Don't write 'import logging'. Just add logging.\n",
    "                Also maintain the indentation of the code snippet.\n",
    "                \"\"\",\n",
    "                paramcount=paramcount\n",
    "            )\n",
    "        self.rectify_code_llama_engine = CodeLLamaInstructEngine(\n",
    "                systemPromptText=\"\"\"\n",
    "                You are an AI assistant. You will be given python function code snippet with logging added.\n",
    "                \n",
    "                You have to check if there is any logging after return statement in the function that is unreachable.\n",
    "                You have to rellocate such logging before each return statement in the function.\n",
    "\n",
    "                Don't modify the rest of the function code snippet.\n",
    "                \"\"\",\n",
    "                paramcount=paramcount)\n",
    "\n",
    "    @DeprecationWarning \n",
    "    def create_user_prompt(self, function_code_snippet, start_log_message, end_log_message):\n",
    "        return f\"\"\"\n",
    "        The function where the logging should be added is:\n",
    "        ```\n",
    "        {function_code_snippet}\n",
    "        ```\n",
    "\n",
    "        The log message to be added at beggining of the function body is:\n",
    "        ```\n",
    "        {start_log_message}\n",
    "        ```\n",
    "\n",
    "        The log message to be added before all return statements is (if there is no return statement, then add at the end of the function):\n",
    "        ```\n",
    "        {end_log_message}\n",
    "        ```\n",
    "\n",
    "        DON'T add any logging AFTER the return statement , always ADD BEFORE the return statement.\n",
    "        \"\"\"\n",
    "    \n",
    "    @DeprecationWarning\n",
    "    def create_rectify_user_prompt(self, function_code_snippet):\n",
    "        return f\"\"\"\n",
    "        The function where the logging should be rectified is:\n",
    "        ```\n",
    "        {function_code_snippet}\n",
    "        ```\n",
    "\n",
    "        You have to check if there is any logging after return statement in the function that is unreachable.\n",
    "        You have to rellocate such logging before each return statement in the function.\n",
    "        \"\"\"\n",
    "    \n",
    "    def add_logging_to_function(self,function_code_snippet, start_log_message_snippet, end_log_message_snippet):\n",
    "        # Add logging at the beginning of the function body\n",
    "        second_line = function_code_snippet.splitlines()[1]\n",
    "        second_line_ws = len(second_line) - len(second_line.lstrip())\n",
    "        intended_start_log_message_snippet = f\"\\n{' '*second_line_ws}{start_log_message_snippet}\\n\"\n",
    "        function_code_snippet = function_code_snippet.splitlines()[0] + intended_start_log_message_snippet + \"\\n\".join(function_code_snippet.splitlines()[1:])\n",
    "\n",
    "        # Add logging before all return statements\n",
    "        reconstructed_function_code_snippet = \"\"\n",
    "        for line in function_code_snippet.splitlines():\n",
    "            if line.strip().startswith(\"return\"):\n",
    "                ws = len(line) - len(line.lstrip())\n",
    "                reconstructed_function_code_snippet += f\"\\n{' '*ws}{end_log_message_snippet}\\n{line}\\n\"\n",
    "            else:\n",
    "                reconstructed_function_code_snippet += f\"\\n{line}\"\n",
    "            \n",
    "        return reconstructed_function_code_snippet\n",
    "    \n",
    "    def modify_functions(self,function_code_snippets):\n",
    "        old_and_new_code_snippets = {}\n",
    "        hash_values = []\n",
    "        for function in function_code_snippets:\n",
    "            print(f\"Log: Modifying function: {function.splitlines()[0]}\")\n",
    "            hash_value = hash(function)\n",
    "            logging_start = f\"logging.info('<START{hash_value}>')\"\n",
    "            logging_end = f\"logging.info('<END{hash_value}>')\"\n",
    "\n",
    "            # # LLM INVOKATION START\n",
    "            # user_prompt = self.create_user_prompt(function, logging_start, logging_end)\n",
    "            # response = self.embedder_code_llama_engine.run(user_prompt)\n",
    "            # # LLM INVOKATION END\n",
    "\n",
    "            # Manually adding logging to the function\n",
    "            response = self.add_logging_to_function(function, logging_start, logging_end)\n",
    "\n",
    "            # print(\"----- Original Function -----\")\n",
    "            # print(function)\n",
    "            # print(\"----- Modified Function -----\")\n",
    "            # print(response)\n",
    "\n",
    "            old_and_new_code_snippets[function] = response\n",
    "            hash_values.append(hash_value)\n",
    "\n",
    "        return old_and_new_code_snippets,hash_values\n",
    "\n",
    "    def hash_value_to_line_number(self, file_content, old_and_new_code_snippets, relative_filepath):\n",
    "        hash_to_lineno = {}\n",
    "        for old_code_snip, new_code_snip in old_and_new_code_snippets.items():\n",
    "            hash_value = hash(old_code_snip)\n",
    "            # Starting index where old_code_snip is present in the file_content\n",
    "            start_index = file_content.find(old_code_snip)\n",
    "            # Ending index where old_code_snip is present in the file_content\n",
    "            end_index = start_index + len(old_code_snip)\n",
    "            \n",
    "            # Find the line number of the starting index\n",
    "            start_index_lineno = file_content[:start_index].count(\"\\n\")\n",
    "            # Find the line number of the ending index\n",
    "            end_index_lineno = file_content[:end_index].count(\"\\n\")\n",
    "            \n",
    "            hash_to_lineno[hash_value] = [start_index_lineno, end_index_lineno, relative_filepath]\n",
    "        return hash_to_lineno\n",
    "\n",
    "    def run(self, read_filepath, write_filepath, logging_filepath):\n",
    "        \"\"\"\n",
    "        read_filepath: str: The path of the original file to read\n",
    "        write_filepath: str: The path of the file to write the modified content\n",
    "        logging_filepath: str: The path of the logging file to write the logging content when the code is executed\n",
    "        \"\"\"\n",
    "        # Read the content of the file\n",
    "        logging_header = f\"import logging\\nlogging.basicConfig(filename='{logging_filepath}', level=logging.INFO, format='%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\"\n",
    "        file_content, code_snippets = get_functions_and_classes(read_filepath)\n",
    "        # Modify the functions\n",
    "        old_and_new_code_snippets,hash_values = self.modify_functions(code_snippets[\"functions\"])\n",
    "\n",
    "        # Hash value to line number mapping\n",
    "        hash_to_lineno = self.hash_value_to_line_number(file_content, old_and_new_code_snippets, read_filepath)\n",
    "\n",
    "        # Add logging header to the file content\n",
    "        file_content = f\"{logging_header}\\n{file_content}\"\n",
    "        # Replace the old code with the new code in the file content\n",
    "        for old_code_snip, new_code_snip in old_and_new_code_snippets.items():\n",
    "            hash_value = hash(old_code_snip)\n",
    "            file_content = replace_code_in_file(file_content, old_code_snip, new_code_snip,handle_indentation=False) # handle_indentation is False because indentation is not altered in the modified code\n",
    "\n",
    "        # Write the modified content to the file\n",
    "        write_code(write_filepath, file_content)\n",
    "        print(\"Log: Done\")\n",
    "\n",
    "        return old_and_new_code_snippets,hash_to_lineno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Ananlyze Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A function to read body inside if __name__ == '__main__': block\n",
    "\n",
    "def read_main_block(filepath):\n",
    "    with open(filepath, \"r\") as file:\n",
    "        file_content = file.read()\n",
    "    tree = ast.parse(file_content)\n",
    "    main_block = \"\"\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.If):\n",
    "            if isinstance(node.test, ast.Compare):\n",
    "                if isinstance(node.test.left, ast.Name) and isinstance(node.test.comparators[0], ast.Str):\n",
    "                    if node.test.left.id == \"__name__\" and node.test.comparators[0].s == \"__main__\":\n",
    "                        start_line = node.lineno - 1\n",
    "                        end_line = node.end_lineno\n",
    "                        main_block = \"\".join(file_content.splitlines(keepends=True)[start_line:end_line])\n",
    "    return main_block\n",
    "\n",
    "def retireve_all_parse_args(filepath):\n",
    "    \"\"\"\n",
    "    It takes a file path and returns the arguments of the parser in the file\n",
    "    returns: A List of dictionaries containing the arguments of the parser\n",
    "    [\n",
    "        {\n",
    "            \"name\":\"arg1\",\n",
    "            \"type\":\"int\",\n",
    "            \"default\":None,\n",
    "            \"required\":True,\n",
    "            \"help\":\"This is arg1\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    main_block = read_main_block(filepath)\n",
    "    tree = ast.parse(main_block)\n",
    "    parse_args = []\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Call):\n",
    "            if isinstance(node.func, ast.Attribute):\n",
    "                if isinstance(node.func.value, ast.Name) and node.func.value.id == \"parser\" and node.func.attr == \"add_argument\":\n",
    "                    arg_name = node.args[0].s\n",
    "                    arg_type = node.keywords[0].value.id\n",
    "                    arg_default = None\n",
    "                    arg_required = True\n",
    "                    arg_help = None\n",
    "                    for keyword in node.keywords:\n",
    "                        if keyword.arg == \"default\":\n",
    "                            arg_default = keyword.value.s\n",
    "                        if keyword.arg == \"required\":\n",
    "                            arg_required = keyword.value.value\n",
    "                        if keyword.arg == \"help\":\n",
    "                            arg_help = keyword.value.s\n",
    "                    parse_args.append({\n",
    "                        \"name\":arg_name.split(\"--\")[1],\n",
    "                        \"type\":arg_type,\n",
    "                        \"default\":arg_default,\n",
    "                        \"required\":arg_required,\n",
    "                        \"help\":arg_help\n",
    "                    })\n",
    "    return parse_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_project_folder(folderpath):\n",
    "    \"\"\"\n",
    "    It takes a folder path and processes all the python files in the folder\n",
    "    \"\"\"\n",
    "    # Extract project name from folder path\n",
    "    project_name = folderpath.split(\"/\")[2]\n",
    "    # Creating a folder to store the modified files\n",
    "    try:\n",
    "        os.mkdir(f\"./interim_projects/{project_name}\")\n",
    "    except FileExistsError:\n",
    "        print(\"Interim folder already exists\")\n",
    "\n",
    "    # Creating log embedding workflow object\n",
    "    log_embedder = LogEmbedderWorkflow(paramcount=7)\n",
    "                                \n",
    "    # Dictionary to store hash value to line number mapping for all the files in the project\n",
    "    hash_to_lineno_fullproj = {}\n",
    "    # Dictionary to store the arguments of the parser in all the files in the project\n",
    "    parse_args_fullproj = {}\n",
    "    \n",
    "     # Get all the python files in the folder\n",
    "    python_files = glob.glob(f\"{folderpath}/*.py\")\n",
    "    for file in python_files:\n",
    "        read_filepath = file\n",
    "        write_filepath = f\"./interim_projects/{project_name}/{file.split('/')[-1]}\"\n",
    "        logging_filepath = f\"./function_logs/{project_name}.log\"\n",
    "        print(f\"Read File: {read_filepath} | Write File: {write_filepath} | Logging File: {logging_filepath}\")\n",
    "        # Process the file\n",
    "        old_and_new_code_snippets,hash_to_lineno = log_embedder.run(read_filepath, write_filepath, logging_filepath)\n",
    "        hash_to_lineno_fullproj.update(hash_to_lineno)\n",
    "\n",
    "        parse_args = retireve_all_parse_args(read_filepath)\n",
    "        parse_args_fullproj[read_filepath] = parse_args\n",
    "\n",
    "        print(\"Log: Done\")\n",
    "    \n",
    "    return hash_to_lineno_fullproj,parse_args_fullproj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _log_embedder_workflow = LogEmbedderWorkflow(paramcount=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _old_and_new_code_snippets,_hash_to_lineno = _log_embedder_workflow.run(\n",
    "#     read_filepath=\"./target_project/prog3.py\",\n",
    "#     write_filepath=\"./target_project/prog3_mod.py\",\n",
    "#     logging_filepath=\"./function_logs/hello_world.log\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _parse_args = retireve_all_parse_args(\"./target_project/prog3_mod.py\")\n",
    "# for arg in _parse_args:\n",
    "#     print(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interim folder already exists\n",
      "Read File: ./target_projects/proj1/lib1.py | Write File: ./interim_projects/proj1/lib1.py | Logging File: ./function_logs/proj1.log\n",
      "Log: Modifying function: def cpu_bound(p):\n",
      "Log: Modifying function: def memory_bound(p):\n",
      "Log: Modifying function: def io_bound(p):\n",
      "Log: Modifying function: def power_bound():\n",
      "Log: Done\n",
      "Log: Done\n",
      "Read File: ./target_projects/proj1/lib2.py | Write File: ./interim_projects/proj1/lib2.py | Logging File: ./function_logs/proj1.log\n",
      "Log: Modifying function: def bubble_sort(arr):\n",
      "Log: Modifying function: def selection_sort(arr):\n",
      "Log: Modifying function: def python_sort(arr):\n",
      "Log: Modifying function: def generate_random_list(power_of_ten):\n",
      "Log: Done\n",
      "Log: Done\n",
      "Read File: ./target_projects/proj1/main.py | Write File: ./interim_projects/proj1/main.py | Logging File: ./function_logs/proj1.log\n",
      "Log: Done\n",
      "Log: Done\n"
     ]
    }
   ],
   "source": [
    "_hash_to_lineno_fullproj, _parse_args_fullproj = process_project_folder(\"./target_projects/proj1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{17049: [4, 14, './target_projects/proj1/lib1.py'],\n",
       " 19555: [16, 26, './target_projects/proj1/lib1.py'],\n",
       " 41611: [28, 47, './target_projects/proj1/lib1.py'],\n",
       " 9996: [49, 54, './target_projects/proj1/lib1.py'],\n",
       " 13634: [3, 10, './target_projects/proj1/lib2.py'],\n",
       " 19055: [12, 21, './target_projects/proj1/lib2.py'],\n",
       " 4247: [23, 26, './target_projects/proj1/lib2.py'],\n",
       " 9412: [28, 30, './target_projects/proj1/lib2.py']}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_hash_to_lineno_fullproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'./target_projects/proj1/lib1.py': [],\n",
       " './target_projects/proj1/lib2.py': [],\n",
       " './target_projects/proj1/main.py': [{'name': 'cpu_power',\n",
       "   'type': 'int',\n",
       "   'default': None,\n",
       "   'required': True,\n",
       "   'help': 'Power of CPU bound task'},\n",
       "  {'name': 'memory_power',\n",
       "   'type': 'int',\n",
       "   'default': None,\n",
       "   'required': True,\n",
       "   'help': 'Power of memory bound task'},\n",
       "  {'name': 'io_power',\n",
       "   'type': 'int',\n",
       "   'default': None,\n",
       "   'required': True,\n",
       "   'help': 'Power of I/O bound task'},\n",
       "  {'name': 'power_of_10',\n",
       "   'type': 'int',\n",
       "   'default': None,\n",
       "   'required': True,\n",
       "   'help': 'Power of 10 for sorting list'}]}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_parse_args_fullproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
